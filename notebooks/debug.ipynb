{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from loguru import logger\n",
    "from unsloth import FastLanguageModel, PatchFastRL, is_bfloat16_supported\n",
    "\n",
    "import wandb\n",
    "\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer  # need to import after unsloth patched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from r1rl.datasets import GSM8kDataset\n",
    "\n",
    "dataset_class = GSM8kDataset\n",
    "dataset = dataset_class(\n",
    "    is_chat=False,\n",
    "    shuffle_seed=3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"Qwen/Qwen2.5-3B\",\n",
    "    max_seq_length=1024,\n",
    "    load_in_4bit=True,  # False for LoRA 16bit\n",
    "    fast_inference=True,  # Enable vLLM fast inference\n",
    "    max_lora_rank=64,\n",
    "    gpu_memory_utilization=0.6,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # Remove QKVO if out of memory\n",
    "    lora_alpha=64,\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Enable long context finetuning\n",
    "    random_state=3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = GRPOConfig(\n",
    "    use_vllm=True,  # use vLLM for fast inference!\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    logging_steps=1,\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_generations=4,\n",
    "    max_prompt_length=250,\n",
    "    max_completion_length=500,\n",
    "    max_steps=500,\n",
    "    eval_on_start=False,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=250,\n",
    "    save_steps=250,\n",
    "    max_grad_norm=0.1,\n",
    "    report_to=\"none\",\n",
    "    output_dir=f\"../dumps/outputs_tmp\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from r1rl.utils import extract_answer\n",
    "\n",
    "\n",
    "# Reward functions\n",
    "def correctness_reward_func(\n",
    "    completions: List[List[Dict[str, str]] | List[str]],\n",
    "    *,\n",
    "    prompts: List[List[Dict[str, str]] | List[str]],\n",
    "    answer: List[str],\n",
    "    question: List[str],\n",
    "    **kwargs,\n",
    ") -> list[float]:\n",
    "    def get_completion_content(completion: List[Dict[str, str]] | str) -> str:\n",
    "        if isinstance(completion, str):\n",
    "            return completion\n",
    "        else:\n",
    "            return completion[0][\"content\"]\n",
    "\n",
    "    responses = [get_completion_content(completion) for completion in completions]\n",
    "    extracted_responses = [extract_answer(r) for r in responses]\n",
    "\n",
    "    infos = {\n",
    "        \"Prompt\": prompts[0],\n",
    "        \"Question\": question[0],\n",
    "        \"Response\": responses[0],\n",
    "        \"Ground Truth\": answer[0],\n",
    "        \"Extracted\": \"\\n\".join(\n",
    "            [f\"{i + 1}. {r}\" for i, r in enumerate(extracted_responses)]\n",
    "        ),\n",
    "    }\n",
    "    logger.info(\"\\n\".join([f\"{k}:\\n{v}\" for k, v in infos.items()]))\n",
    "    return [1.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[correctness_reward_func],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset.train_dataset,\n",
    "    eval_dataset=dataset.eval_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
